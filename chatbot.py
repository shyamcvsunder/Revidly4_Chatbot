# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-7eW4IqRa0uZtcMPGDUs0mVWm6yst-RH
"""

import nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import json
import pickle

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD
import random



nltk.download('punkt')

nltk.download('wordnet')

words=[]
classes = []
documents = []
ignore_letters = ['!', '?', ',', '.']
data_file = open('intents2.json').read()
intents = json.loads(data_file)

for intent in intents['intents']:
    for pattern in intent['patterns']:

        #tokenize each word
        w = nltk.word_tokenize(pattern)
        print("w",w)
        words.extend(w)
        print("words",words)
        #add documents in the corpus
        documents.append((w, intent['tag']))
        print("documnets",documents)
        # add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])
        print("classes",classes)

# lemmaztize and lower each word and remove duplicates
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
print("1",words)
words = sorted(list(set(words)))
print("2",words)
# sort classes
classes = sorted(list(set(classes)))
# documents = combination between patterns and intents
print (len(documents), "documents")
# classes = intents
print (len(classes), "classes", classes)
# words = all words, vocabulary
print (len(words), "unique lemmatized words", words)

words

# create our training data
training = []
# create an empty array for our output
output_empty = [0] * len(classes)
print(output_empty)
# training set, bag of words for each sentence
for doc in documents:
    # initialize our bag of words
    bag = []
    # list of tokenized words for the pattern
    pattern_words = doc[0]
    print("p1",pattern_words)
    # lemmatize each word - create base word, in attempt to represent related words
    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
    print("p2",pattern_words)
    # create our bag of words array with 1, if word match found in current pattern
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)
    
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    
    training.append([bag, output_row])
# shuffle our features and turn into np.array
random.shuffle(training)
print("randaom",training)
training = np.array(training)
# create train and test lists. X - patterns, Y - intents
train_x = list(training[:,0])
print("train_x",train_x)
train_y = list(training[:,1])
print("train_y",train_y)
print("Training data created")

maxLen = len(train_x)

#Another Model 
#from keras.models import Sequential, Model
# from keras.layers.embeddings import Embedding
# from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM
# max_question_len = 20
# batch_size = 32
# #Example of a Placeholder
# #input_sequence = Input((max_story_len,))
# question = Input((max_question_len,))
# #Create input encoder A:
# input_encoder_m = Sequential()
# input_encoder_m.add(Embedding(input_dim=len(words)+1,output_dim = 64)) 
# input_encoder_m.add(Dropout(0.3))
# #Outputs: (Samples, story_maxlen,embedding_dim) -- Gives a list of #the lenght of the samples where each item has the
# #lenght of the max story lenght and every word is embedded in the embbeding dimension
# match = dot([input_encoded_m,question_encoded], axes = (2,2))
# match = Activation('softmax')(match)
# response = add([match,input_encoded_c])
# response = Permute((2,1))(response)
# answer = concatenate([response, question_encoded])
# answer = LSTM(32)(answer)
# answer = Dropout(0.5)(answer)
# #Output layer:
# answer = Dense(vocab_len)(answer) 
# #Output shape: (Samples, Vocab_size) #Yes or no and all 0s
# answer = Activation('softmax')(answer)
# model = Model([input_sequence,question], answer)
# model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons
# equal to number of intents to predict output intent with softmax
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1,validation_split=0.3)

model.predict()

